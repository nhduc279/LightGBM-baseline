If your daily job is to work with tabular data. After going through the steps of data collection, data manipulation, data visualization, etc. Perhaps the part that we are often excited about is model building.

If you’re new or experienced with ML, you probably use models that quickly give you decent results such as Logistic Regression, Decision Trees, Random Forest, Bagging, K-Nearest Neighbors, Naive Bayes, etc. All of them are good. In terms of speed of training and inference, you might prefer Logistic Regression, K-Nearest Neighbors, and Naive Bayes.

On Kaggle or at some data science competitions, several models emerge as highly competitive models on tabular data, namely XGBoost, CatBoost, HistGB and LightGBM. Each has its own perks. But to speed, LightGBM is considered as no.1 candidate.

So, I have decided to share how I build a LightGBM baseline that might help you:

- Quickly to have a good solution.
- Arm you with a decent baseline to iterate your ideas for days/weeks/even months.
- Set up a proper CV strategy
- Create an out-of-fold
- Examine the feature importance
- Examine the out-of-fold

Below is the notebook that uses a real-world dataset from Vietnam World Bank Livings Standards Survey. There is some parts of a typical DS workflow such as doing some EDA, data preprocessing, and model building for a regression problem using LightGBM model :robot:

:point_right: **Notebook**: https://github.com/nhduc279/LightGBM-baseline/blob/main/LightGBM_baseline.ipynb

If you do get stuck or have a small misunderstanding in my notebook, maybe I’ve worded and coded something poorly. Please feel free to <a href = "mailto: abc@example.com">send me an email</a>, I will definitely strive to answer those.

Thanks,
Duc

P/s: Of course, deep learning can beat LightGBM, but in the real world, **time is money**!
